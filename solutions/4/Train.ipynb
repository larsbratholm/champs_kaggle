{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SEUbn4AM4a6J"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pdb\n",
    "import math\n",
    "import dgl\n",
    "import torch\n",
    "import weakref\n",
    "import numbers\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import cytoolz.curried as ct\n",
    "import humanfriendly as hf\n",
    "import itertools\n",
    "\n",
    "import dgl.function as fn\n",
    "import dgl.nn.pytorch as dglnn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.tensorboard as tb\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import subprocess\n",
    "\n",
    "\n",
    "SEED = 43\n",
    "SCALED = True\n",
    "TRUE_MLKN = True\n",
    "\n",
    "TYPES_TO_PROCESS = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "# ['1JHC', '1JHN', '2JHC', '2JHH', '2JHN', '3JHC', '3JHH', '3JHN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QFrHK4Ic-KkW"
   },
   "outputs": [],
   "source": [
    "settings = json.load(open('SETTINGS.json'))\n",
    "\n",
    "RUNS = settings['TRAIN']['TBOARD_LOG']\n",
    "CHKPS = settings['TRAIN']['MODEL_CHKPS']\n",
    "prefix = settings['TRAIN']['MODEL_NAME']\n",
    "dataset = settings['TRAIN']['INPUT']\n",
    "mlk = settings['TRAIN']['Q9MLK']\n",
    "START = settings['TRAIN']['START_FROM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GCtuE5Xjpufw"
   },
   "outputs": [],
   "source": [
    "# RUNS = \"gdrive/My Drive/CHAMPS/runs\"\n",
    "# DATA  = 'gdrive/My Drive/CHAMPS/champs/final/input_data'\n",
    "# CHKPS = 'gdrive/My Drive/CHAMPS//checkpoints'\n",
    "#prefix = settings['TRAIN']['MODEL_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cImt75O-jlNY"
   },
   "outputs": [],
   "source": [
    "gdata = torch.load(dataset)\n",
    "if TRUE_MLKN:\n",
    "    gdata_true = torch.load(mlk)\n",
    "    for i in range(len(gdata)):\n",
    "        assert len(gdata[i]['ndata']['mulliken']) == len(gdata_true[i]['mulliken']), i\n",
    "        gdata[i]['ndata']['mulliken'] = gdata_true[i]['mulliken']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 74558,
     "status": "ok",
     "timestamp": 1567683310749,
     "user": {
      "displayName": "Goran Rakocevic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUktqkpJY0qFKtpI6DuRekptNOOnmdXwvcQokR=s64",
      "userId": "16829807710678167120"
     },
     "user_tz": -120
    },
    "id": "2G5EeJqqjlNf",
    "outputId": "65a5f5bf-adf5-44ef-ea14-27d7c3409341"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8653"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.RandomState(seed=SEED)\n",
    "is_eval = rng.rand(len(gdata)) > .9\n",
    "train_data = [g for g, e in zip(gdata, is_eval) if not e]\n",
    "eval_data = [g for g, e in zip(gdata, is_eval) if e]\n",
    "assert (len(train_data) + len(eval_data)) == len(gdata)\n",
    "len(eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j2M7EMWx8eGJ"
   },
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XIqJoamFjlNl"
   },
   "outputs": [],
   "source": [
    "class LambW(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6,\n",
    "                 weight_decay=0, adam=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay)\n",
    "        self.adam = adam\n",
    "        super(LambW, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Lamb does not support sparse gradients, consider SparseAdam instad.')\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                step_size = group['lr']\n",
    "\n",
    "                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n",
    "                adam_step = exp_avg / exp_avg_sq.sqrt().add(group['eps'])\n",
    " \n",
    "                adam_norm = adam_step.pow(2).sum().sqrt()\n",
    "                if weight_norm == 0 or adam_norm == 0:\n",
    "                    trust_ratio = 1\n",
    "                else:\n",
    "                    trust_ratio = weight_norm / adam_norm\n",
    "                state['weight_norm'] = weight_norm\n",
    "                state['adam_norm'] = adam_norm\n",
    "                state['trust_ratio'] = trust_ratio\n",
    "                if self.adam:\n",
    "                    trust_ratio = 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p.data.add_(-group['weight_decay'] * group['lr'], p.data)\n",
    "                p.data.add_(-step_size * trust_ratio, adam_step)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "class CyclicLR(torch.optim.lr_scheduler._LRScheduler):\n",
    "    \"\"\"Taken form Pytorch code to patch a bug when working with Adam.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 optimizer,\n",
    "                 base_lr,\n",
    "                 max_lr,\n",
    "                 step_size_up=2000,\n",
    "                 step_size_down=None,\n",
    "                 mode='triangular',\n",
    "                 gamma=1.,\n",
    "                 scale_fn=None,\n",
    "                 scale_mode='cycle',\n",
    "                 cycle_momentum=True,\n",
    "                 base_momentum=0.8,\n",
    "                 max_momentum=0.9,\n",
    "                 last_epoch=-1):\n",
    "\n",
    "        if not isinstance(optimizer, torch.optim.Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        base_lrs = self._format_param('base_lr', optimizer, base_lr)\n",
    "        if last_epoch == -1:\n",
    "            for lr, group in zip(base_lrs, optimizer.param_groups):\n",
    "                group['lr'] = lr\n",
    "\n",
    "        self.max_lrs = self._format_param('max_lr', optimizer, max_lr)\n",
    "\n",
    "        step_size_up = float(step_size_up)\n",
    "        step_size_down = float(step_size_down) if step_size_down is not None else step_size_up\n",
    "        self.total_size = step_size_up + step_size_down\n",
    "        self.step_ratio = step_size_up / self.total_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.cycle_momentum = cycle_momentum\n",
    "        if cycle_momentum:\n",
    "\n",
    "            base_momentums = self._format_param('base_momentum', optimizer, base_momentum)\n",
    "            if last_epoch == -1:\n",
    "                for momentum, group in zip(base_momentums, optimizer.param_groups):\n",
    "                    group['betas'] = (group['betas'][0], momentum)\n",
    "            self.base_momentums = list(map(lambda group: group['betas'][1], optimizer.param_groups))\n",
    "            self.max_momentums = self._format_param('max_momentum', optimizer, max_momentum)\n",
    "\n",
    "        super(CyclicLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def _format_param(self, name, optimizer, param):\n",
    "        \"\"\"Return correctly formatted lr/momentum for each param group.\"\"\"\n",
    "        if isinstance(param, (list, tuple)):\n",
    "            if len(param) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} values for {}, got {}\".format(\n",
    "                    len(optimizer.param_groups), name, len(param)))\n",
    "            return param\n",
    "        else:\n",
    "            return [param] * len(optimizer.param_groups)\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"Calculates the learning rate at batch index. This function treats\n",
    "        `self.last_epoch` as the last batch index.\n",
    "\n",
    "        If `self.cycle_momentum` is ``True``, this function has a side effect of\n",
    "        updating the optimizer's momentum.\n",
    "        \"\"\"\n",
    "        cycle = math.floor(1 + self.last_epoch / self.total_size)\n",
    "        x = 1. + self.last_epoch / self.total_size - cycle\n",
    "        if x <= self.step_ratio:\n",
    "            scale_factor = x / self.step_ratio\n",
    "        else:\n",
    "            scale_factor = (x - 1) / (self.step_ratio - 1)\n",
    "\n",
    "        lrs = []\n",
    "        for base_lr, max_lr in zip(self.base_lrs, self.max_lrs):\n",
    "            base_height = (max_lr - base_lr) * scale_factor\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_epoch)\n",
    "            lrs.append(lr)\n",
    "\n",
    "        if self.cycle_momentum:\n",
    "            momentums = []\n",
    "            for base_momentum, max_momentum in zip(self.base_momentums, self.max_momentums):\n",
    "                base_height = (max_momentum - base_momentum) * scale_factor\n",
    "                if self.scale_mode == 'cycle':\n",
    "                    momentum = max_momentum - base_height * self.scale_fn(cycle)\n",
    "                else:\n",
    "                    momentum = max_momentum - base_height * self.scale_fn(self.last_epoch)\n",
    "                momentums.append(momentum)\n",
    "            for param_group, momentum in zip(self.optimizer.param_groups, momentums):\n",
    "                param_group['betas'] = (param_group['betas'][0], momentum)\n",
    "\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eHk96RRU8iqn"
   },
   "source": [
    "# Code for Stohastic Weight Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cUfOsQvN4a7D"
   },
   "outputs": [],
   "source": [
    "class MathDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MathDict, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def __op__(self, other, op):\n",
    "        if isinstance(other, dict):\n",
    "            return MathDict({ k: op(v, other[k]) for k, v in self.items() })\n",
    "        if isinstance(other, (numbers.Number, torch.Tensor)):\n",
    "            return MathDict({ k: op(v, other) for k, v in self.items() })\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        return self.__op__(other, op=operator.add)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        return self.__op__(other, op=operator.mul)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self.__op__(other, op=operator.sub)\n",
    "    \n",
    "    def __mod__(self, other):\n",
    "        return self.__op__(other, op=operator.mod)\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self.__op__(other, op=operator.truediv)\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        return self.__op__(other, op=operator.lt)\n",
    "    \n",
    "    def __le__(self, other):\n",
    "        return self.__op__(other, op=operator.le)\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        return self.__op__(other, op=operator.gt)\n",
    "    \n",
    "    def __ge__(self, other):\n",
    "        return self.__op__(other, op=operator.ge)\n",
    "    \n",
    "class SWA(object):\n",
    "    def __init__(self):\n",
    "        super(SWA, self).__init__()\n",
    "        self.wswa = None\n",
    "        self.nmodels = 0\n",
    "    \n",
    "    def add_model(self, model):\n",
    "        if self.nmodels == 0:\n",
    "            self.wswa = MathDict(model)\n",
    "            self.nmodels = 1\n",
    "            return\n",
    "        self.wswa = (self.wswa * self.nmodels + model) / (self.nmodels + 1)\n",
    "        self.nmodels += 1\n",
    "\n",
    "@ct.curry\n",
    "def last_n_swa(n, end_epoch, checkpoints_path, device=None):\n",
    "    base = Path(checkpoints_path)\n",
    "    epochs = range(end_epoch-n+1, end_epoch+1)\n",
    "    swa = SWA()\n",
    "    for epoch in epochs:\n",
    "        chk = torch.load(base / f'{epoch}.torch', map_location=device)\n",
    "        swa.add_model(chk['model'])\n",
    "    chk['model'] = swa.wswa\n",
    "    chk['swa_nmodels'] = swa.nmodels\n",
    "    return chk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c8GIP9tq8rCE"
   },
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0r7CQwab4a7O"
   },
   "outputs": [],
   "source": [
    "def make_dglg(mol, precision='double'):\n",
    "    g = dgl.DGLGraph()\n",
    "    nodes = mol['nodes']\n",
    "    g.add_nodes(nodes)\n",
    "    src, dst = mol['src'], mol['dst']\n",
    "    g.ndata.update(mol['ndata'])\n",
    "    g.ndata['type'] = g.ndata['type']\n",
    "    g.add_edges(src, dst)\n",
    "    g.add_edges(dst, src)\n",
    "    g.add_edges(range(nodes), range(nodes))\n",
    "    \n",
    "    device = src.device\n",
    "    zeros = torch.zeros(nodes, dtype=torch.float32, device=device)\n",
    "    ones = torch.ones(nodes, dtype=torch.int64, device=device) \n",
    "    edata = {}\n",
    "    edata['type'] = torch.cat([mol['edata']['type'].repeat(2), ones * 6])\n",
    "    edata['distance'] = torch.cat([mol['edata']['distance'].repeat(2), zeros])\n",
    "    edata['angle'] = torch.cat([mol['edata']['angle'].repeat(2), zeros])\n",
    "    edata['dihedral'] = torch.cat([mol['edata']['dihedral'], -mol['edata']['dihedral'], zeros])\n",
    "    edata['coupling_type'] = torch.cat([mol['edata']['coupling_type'].repeat(2).to(torch.int64), -ones])\n",
    "    edata['coupling'] = torch.cat([mol['edata']['coupling'].repeat(2), zeros])\n",
    "    edata['train_id'] = torch.cat([mol['edata']['train_id'].repeat(2), -ones])\n",
    "    edata['test_id'] = torch.cat([mol['edata']['test_id'].repeat(2), -ones])\n",
    "\n",
    "    g.edata.update(edata)\n",
    "    return g\n",
    "\n",
    "  \n",
    "@ct.curry\n",
    "def make_graph_batch(mols, precision='double'):\n",
    "    g = dgl.batch([make_dglg(mol, precision) for mol in mols])\n",
    "    g.set_n_initializer(dgl.init.zero_initializer)\n",
    "    g.set_e_initializer(dgl.init.zero_initializer)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w9Cz6UMW8yIG"
   },
   "source": [
    "# Layers for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rN1DTva34a7W"
   },
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    \n",
    "    def __init__(self, node_dim, edge_dim):\n",
    "        super(Embed, self).__init__()\n",
    "        self.emb_node_types = nn.Embedding(10, node_dim-4)\n",
    "        self.emb_edge_types = nn.Embedding(10, edge_dim-3)\n",
    "        \n",
    "    def forward(self, g):\n",
    "        emb = self.emb_node_types(g.ndata['type'])\n",
    "        g.ndata['emb'] = torch.cat([\n",
    "            emb,\n",
    "            g.ndata['el_aff'].unsqueeze(1),\n",
    "            g.ndata['el_neg'].unsqueeze(1),\n",
    "            g.ndata['1st_ion'].unsqueeze(1),\n",
    "            g.ndata['mulliken'].unsqueeze(1)\n",
    "        ], dim=-1)\n",
    "        \n",
    "        emb = self.emb_edge_types(g.edata['type'])\n",
    "        g.edata['emb'] = torch.cat([\n",
    "            emb,\n",
    "            g.edata['distance'].unsqueeze(1),\n",
    "            g.edata['angle'].unsqueeze(1),\n",
    "            g.edata['dihedral'].unsqueeze(1),\n",
    "        ], dim=-1)\n",
    "        return g\n",
    "      \n",
    "\n",
    "class GraphCast(nn.Module):\n",
    "    def __init__(self, dtype):\n",
    "        super(GraphAct, self).__init__()\n",
    "        self.dtype = dtype\n",
    "    \n",
    "    def forward(self, g):\n",
    "        g.ndata['emb'] = g.ndata['emb'].to(self.dtype)\n",
    "        g.edata['emb'] = g.edata['emb'].to(self.dtype)\n",
    "        return g\n",
    "      \n",
    "    \n",
    "class EdgeSoftmax(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, g, score):\n",
    "        score_name = dgl.utils.get_edata_name(g, 'score')\n",
    "        tmp_name = dgl.utils.get_ndata_name(g, 'tmp')\n",
    "        out_name = dgl.utils.get_edata_name(g, 'out')\n",
    "        g.edata[score_name] = score\n",
    "        g.update_all(fn.copy_e(score_name, 'm'), fn.max('m', tmp_name))\n",
    "        g.apply_edges(fn.e_sub_v(score_name, tmp_name, out_name))\n",
    "        g.edata[out_name] = torch.exp(g.edata[out_name])\n",
    "        g.update_all(fn.copy_e(out_name, 'm'), fn.sum('m', tmp_name))\n",
    "        g.apply_edges(fn.e_div_v(out_name, tmp_name, out_name))\n",
    "        g.edata.pop(score_name)\n",
    "        g.ndata.pop(tmp_name)\n",
    "        out = g.edata.pop(out_name)\n",
    "        ctx.backward_cache = weakref.ref(g)\n",
    "        ctx.save_for_backward(out)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        g = ctx.backward_cache()\n",
    "        out, = ctx.saved_tensors\n",
    "        # clear backward cache explicitly\n",
    "        ctx.backward_cache = None\n",
    "        out_name = dgl.utils.get_edata_name(g, 'out')\n",
    "        accum_name = dgl.utils.get_ndata_name(g, 'accum')\n",
    "        grad_score_name = dgl.utils.get_edata_name(g, 'grad_score')\n",
    "        g.edata[out_name] = out\n",
    "        g.edata[grad_score_name] = out * grad_out\n",
    "        g.update_all(fn.copy_e(grad_score_name, 'm'), fn.sum('m', accum_name))\n",
    "        g.apply_edges(fn.e_mul_v(out_name, accum_name, out_name))\n",
    "        g.ndata.pop(accum_name)\n",
    "        grad_score = g.edata.pop(grad_score_name) - g.edata.pop(out_name)\n",
    "        return None, grad_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QGg5vmhM4a7g"
   },
   "outputs": [],
   "source": [
    "class Linear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True, init=ct.curry(nn.init.xavier_normal_)(gain=1.414)):\n",
    "        self.init = init\n",
    "        super(Linear, self).__init__(in_features, out_features, bias)\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.init(self.weight)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "    \n",
    "class MultiLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, n_linears=1, bias=True,\n",
    "                init=ct.curry(nn.init.xavier_normal_)(gain=nn.init.calculate_gain('relu'))):\n",
    "        super(MultiLinear, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.n_linears = n_linears\n",
    "        self.in_features = in_features\n",
    "        self.init = init\n",
    "        weights = torch.zeros(n_linears, in_features, out_features, dtype=torch.float32)\n",
    "        init(weights)\n",
    "        self.lin = nn.Parameter(weights)\n",
    "        self.init(self.lin.data)\n",
    "        if bias:\n",
    "            b = torch.zeros((n_linears, 1, self.out_features), dtype=torch.float32)\n",
    "            self.bias = nn.Parameter(b)\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "    def extra_repr(self):\n",
    "        return f'{self.in_features}, {self.out_features}, {self.n_linears}'\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch = x.shape[0]\n",
    "        x = x.view(batch, self.n_linears, -1).permute(1, 0, 2)\n",
    "        if self.bias is not None:\n",
    "            y = torch.baddbmm(self.bias.expand(self.n_linears, batch, self.out_features),\n",
    "                             x,\n",
    "                             self.lin)\n",
    "        else:\n",
    "            y = torch.bmm(input=x,mat2=self.lin.data)\n",
    "        return y.permute(1, 0, 2).contiguous()\n",
    "            \n",
    "class GraphLambda(nn.Module):\n",
    "    def __init__(self, fn, node_key='emb', edge_key='emb'):\n",
    "        super(GraphLambda, self).__init__()\n",
    "        self.fn = fn\n",
    "        self.edge_key = edge_key\n",
    "        self.node_key = node_key\n",
    "    \n",
    "    def forward(self, g):\n",
    "        if self.node_key:\n",
    "            g.ndata[self.node_key] = self.fn(g.ndata[self.node_key])\n",
    "        if self.edge_key:\n",
    "            g.edata[self.edge_key] = self.fn(g.edata[self.edge_key])\n",
    "        return g\n",
    "    \n",
    "ReduceMean = lambda: GraphLambda(lambda x: x.mean(dim=-2))\n",
    "ReduceCat = lambda: GraphLambda(lambda x: x.view(x.shape[0], -1))\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(Residual, self).__init__()\n",
    "        self.module = module\n",
    "    \n",
    "    def forward(self, g):\n",
    "        nemb = g.ndata['emb']\n",
    "        eemb = g.edata['emb']\n",
    "        g = self.module(g)\n",
    "        g.ndata['emb'] += nemb\n",
    "        g.edata['emb'] += eemb\n",
    "        return g\n",
    "    \n",
    "class GatedResidual(nn.Module):\n",
    "    def __init__(self, module, node_dim, edge_dim):\n",
    "        super(GatedResidual, self).__init__()\n",
    "        self.module = module\n",
    "        sig_gain = nn.init.calculate_gain('sigmoid')\n",
    "        self.prev_node_gate = Linear(\n",
    "            node_dim, node_dim, bias=False,\n",
    "            init=ct.curry(nn.init.xavier_normal_)(gain=sig_gain))\n",
    "        self.curr_node_gate = Linear(\n",
    "            node_dim, node_dim, bias=True,\n",
    "            init=ct.curry(nn.init.xavier_normal_)(gain=sig_gain))\n",
    "        self.prev_edge_gate = Linear(\n",
    "            edge_dim, edge_dim, bias=False,\n",
    "            init=ct.curry(nn.init.xavier_normal_)(gain=sig_gain))\n",
    "        self.curr_edge_gate = Linear(\n",
    "            edge_dim, edge_dim, bias=True,\n",
    "            init=ct.curry(nn.init.xavier_normal_)(gain=sig_gain))\n",
    "        nn.init.zeros_(self.curr_node_gate.bias.data)\n",
    "        nn.init.zeros_(self.curr_edge_gate.bias.data)\n",
    "    \n",
    "    def forward(self, g):\n",
    "        prev_node = g.ndata['emb']\n",
    "        prev_edge = g.edata['emb']\n",
    "        \n",
    "        g = self.module(g)\n",
    "        \n",
    "        node_z = torch.sigmoid(\n",
    "            self.prev_node_gate(prev_node) + \\\n",
    "            self.curr_node_gate(g.ndata['emb']))\n",
    "        edge_z = torch.sigmoid(\n",
    "            self.prev_edge_gate(prev_edge) + \\\n",
    "            self.curr_edge_gate(g.edata['emb']))\n",
    "        g.ndata['emb'] = node_z * g.ndata['emb'] + (1 - node_z) * prev_node\n",
    "        g.edata['emb'] = edge_z * g.edata['emb'] + (1 - edge_z) * prev_edge\n",
    "        \n",
    "        return g\n",
    "    \n",
    "class TripletLinear(nn.Module):\n",
    "    def __init__(self, in_node_dim, in_edge_dim, out_edge_dim, bias=False):\n",
    "        super(TripletLinear, self).__init__()\n",
    "        self.lin = Linear(in_node_dim * 2 + in_edge_dim, out_edge_dim, bias)\n",
    "        \n",
    "    def triplet_linear(self, edges):\n",
    "        triplets = torch.cat([edges.src['emb'], edges.data['emb'], edges.dst['emb']], dim=-1)\n",
    "        return { 'triplets' : triplets }\n",
    "    \n",
    "    def forward(self, g):\n",
    "        g.apply_edges(self.triplet_linear)\n",
    "        g.edata['emb'] = self.lin(g.edata.pop('triplets'))\n",
    "        return g\n",
    "\n",
    "      \n",
    "class TripletMultiLinear(nn.Module):\n",
    "    def __init__(self, in_node_dim, in_edge_dim, out_edge_dim, n_lins, bias=False):\n",
    "        super(TripletMultiLinear, self).__init__()\n",
    "        self.lin = MultiLinear(in_node_dim * 2 + in_edge_dim, out_edge_dim, n_lins, bias)\n",
    "        \n",
    "    def triplet_linear(self, edges):\n",
    "        triplets = torch.cat([edges.src['emb'], edges.data['emb'], edges.dst['emb']], dim=-1)\n",
    "        return { 'triplets' : triplets }\n",
    "    \n",
    "    def forward(self, g):\n",
    "        g.apply_edges(self.triplet_linear)\n",
    "        g.edata['emb'] = self.lin(g.edata.pop('triplets'))\n",
    "        return GraphLambda(lambda x: x.view(x.shape[0], -1), node_key=None)(g)\n",
    "\n",
    "\n",
    "class TripletCat(nn.Module):\n",
    "    def __init__(self, out='emb'):\n",
    "        super(TripletCat, self).__init__()\n",
    "        self.out = out\n",
    "\n",
    "    def triplet_linear(self, edges):\n",
    "        triplets = torch.cat([edges.src['emb'], edges.data['emb'], edges.dst['emb']], dim=-1)\n",
    "        return { self.out : triplets }\n",
    "    \n",
    "    def forward(self, g):\n",
    "        g.apply_edges(self.triplet_linear)\n",
    "        return g\n",
    "    \n",
    "\n",
    "class MagicAttn(nn.Module):\n",
    "    def __init__(self, node_dim, edge_dim, n_heads, attn_key='emb', msg_key='emb', alpha=.2):\n",
    "        super(MagicAttn, self).__init__()\n",
    "        self.attn = MultiLinear(\n",
    "            edge_dim, 1, n_heads, bias=False,\n",
    "            init=ct.curry(nn.init.xavier_normal_)(gain=nn.init.calculate_gain('leaky_relu', alpha)))\n",
    "        self.leaky_relu = nn.LeakyReLU(alpha)\n",
    "        self.n_heads = n_heads\n",
    "        self.softmax = EdgeSoftmax.apply\n",
    "        self.attn_key = attn_key\n",
    "        self.msg_key = msg_key\n",
    "        \n",
    "    def forward(self, g):\n",
    "        alpha_prime = self.leaky_relu(self.attn(g.edata[self.attn_key]))\n",
    "        g.edata['a'] = self.softmax(g, alpha_prime) * g.edata['emb'].view(g.edata['emb'].shape[0], self.n_heads, -1)\n",
    "        attn_emb = g.ndata[self.msg_key]\n",
    "        if attn_emb.ndimension() == 2:\n",
    "            g.ndata[self.msg_key] = attn_emb.view(g.number_of_nodes(), self.n_heads, -1)\n",
    "        g.update_all(fn.src_mul_edge(self.msg_key, 'a', 'm'), fn.sum('m', 'emb'))\n",
    "        return GraphLambda(lambda x: x.view(x.shape[0], -1))(g)\n",
    "\n",
    "\n",
    "class HeadExpand(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super(HeadExpand, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "    \n",
    "    def forward(self, g):\n",
    "        g.edata['emb_orig'] = g.edata['emb']\n",
    "        g.ndata['emb_orig'] = g.ndata['emb']\n",
    "        g.edata['emb'] = g.edata['emb'].unsqueeze(1).expand((-1, \n",
    "                                                             self.n_heads, -1)).reshape(g.edata['emb'].shape[0], -1)\n",
    "        g.ndata['emb'] = g.ndata['emb'].unsqueeze(1).expand((-1, \n",
    "                                                             self.n_heads, -1)).reshape(g.ndata['emb'].shape[0], -1)\n",
    "        return g\n",
    "   \n",
    "  \n",
    "class OrigEmb(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OrigEmb, self).__init__()\n",
    "    \n",
    "    def forward(self, g):\n",
    "        g.edata['emb'] = g.edata['emb_orig']\n",
    "        g.ndata['emb'] = g.ndata['emb_orig']\n",
    "        return g\n",
    "\n",
    "      \n",
    "def NodeLinear(in_features, out_features, bias=False):\n",
    "    return GraphLambda(Linear(in_features, out_features, bias), edge_key=None)\n",
    "\n",
    "\n",
    "def EdgeLinear(in_features, out_features, bias=False):\n",
    "    return GraphLambda(Linear(in_features, out_features, bias), node_key=None)\n",
    "  \n",
    "def load_checkpoint(path):\n",
    "    checkpoint = torch.load(path)\n",
    "    global start_epoch, i, prefix, batch_size\n",
    "    net.load_state_dict(checkpoint['model'])\n",
    "    optim.load_state_dict(checkpoint['optim'])\n",
    "    prefix = checkpoint['prefix']\n",
    "    i = checkpoint['iteration']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    if batch_size in checkpoint:\n",
    "        assert batch_size == checkpoint['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OcOSeT3s4a7p"
   },
   "outputs": [],
   "source": [
    "ctype_means = torch.tensor([\n",
    "    94.97615286418801, 47.47988448446838, -0.2706244378832182,\n",
    "    -10.28660516398165, 3.124753613418501,3.6884695895354453,\n",
    "    4.771023359735822, 0.9907298624943462], dtype=torch.float32)\n",
    "ctype_stds = torch.tensor([\n",
    "    18.277236880290143, 10.922171556272271, 4.523610750196489,\n",
    "    3.9796071637303525, 3.6734741723096023, 3.0709074866562185,\n",
    "    3.7049844341285763, 1.3153933535337567], dtype=torch.float32)\n",
    "\n",
    "if not SCALED:\n",
    "    ctype_stds = torch.tensor([1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float32)\n",
    "\n",
    "ctype_means_c = ctype_means.cuda()\n",
    "ctype_stds_c = ctype_stds.cuda()\n",
    "ctype_stds_c_inv = 1/ctype_stds_c\n",
    "\n",
    "\n",
    "def get_outputs(g, id_col='train_id', norm_truth=False, denorm_out=False):\n",
    "    labeled = g.edata['coupling_type'] != -1\n",
    "    out = g.edata['emb'][labeled].squeeze()\n",
    "    truth = g.edata['coupling'][labeled]\n",
    "    ctype = g.edata['coupling_type'][labeled]\n",
    "    \n",
    "    if norm_truth:\n",
    "        truth = (truth - ctype_means_c[ctype])*ctype_stds_c_inv[ctype]\n",
    "      \n",
    "    if denorm_out:\n",
    "        out = out * ctype_stds_c[ctype].unsqueeze(-1) + ctype_means_c[ctype].unsqueeze(-1)\n",
    "      \n",
    "    src, dst = g.all_edges('uv')\n",
    "    src, dst = src[labeled].to(out.device), dst[labeled].to(out.device)\n",
    "    tid = g.edata[id_col][labeled]\n",
    "\n",
    "    return out, ctype, truth, src, dst, tid\n",
    "  \n",
    "  \n",
    "def run_eval(net, tb_writer, epoch, iteration, eval_loader, label='Eval'):\n",
    "    i = iteration\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        outs_n = []\n",
    "        outs_d = []\n",
    "        for batch in tqdm(eval_loader, total=int(np.ceil(len(eval_data) / batch_size)),\n",
    "                          desc=f'{label} epoch: {epoch}', leave=False):\n",
    "            if use_cuda:\n",
    "                batch.to(next(net.parameters()).device)\n",
    "            g = net(batch)\n",
    "            outs_n.append([t.detach().cpu() for t in get_outputs(g, norm_truth=True)])\n",
    "            outs_d.append([t.detach().cpu() for t in get_outputs(g, denorm_out=True)])\n",
    "        \n",
    "        outs_n = zip(*outs_n)\n",
    "        outs_n = [torch.cat(ts) for ts in outs_n]\n",
    "        out, ctype, truth, src, dst, tid = outs_n\n",
    "        loss = loss_fn(out, truth, ctype)\n",
    "        tb_writer.add_scalar('loss', loss.item(), i)\n",
    "        \n",
    "        outs_d = zip(*outs_d)\n",
    "        outs_d = [torch.cat(ts) for ts in outs_d]\n",
    "        out, ctype, truth, src, dst, tid = outs_d\n",
    "        scalars = scalar_metrics(out, ctype, truth, src, dst, tid)\n",
    "        for mlabel, scalar in scalars.items():\n",
    "            tb_writer.add_scalar(mlabel, scalar, i)\n",
    "        tb_writer.file_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j3Q4ro8H9AhH"
   },
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M7kiwYRz4a7u"
   },
   "outputs": [],
   "source": [
    "def logMAE(preds, truth):\n",
    "    return torch.log(F.l1_loss(preds, truth) + 1e-9)\n",
    "  \n",
    "    \n",
    "@ct.curry\n",
    "def logMAE_per_ctype(out, ctypes, truth, ctype_id):\n",
    "    correct_type = ctypes == ctype_id\n",
    "    if not torch.any(correct_type):\n",
    "        return 0\n",
    "    return logMAE(out[correct_type], truth[correct_type]).item()\n",
    "\n",
    "@ct.curry\n",
    "def logMAE_per_ctype_head(out, ctypes, truth, ctype_id):\n",
    "    correct_type = ctypes == ctype_id\n",
    "    if not torch.any(correct_type):\n",
    "        return 0\n",
    "    return logMAE(out[correct_type, ctype_id], truth[correct_type]).item()\n",
    "\n",
    "\n",
    "def mae_head_per_ctype(out, ctypes, truth, ctype_id):\n",
    "    correct_type = ctypes == ctype_id\n",
    "    if not torch.any(correct_type):\n",
    "        return torch.tensor(0.0, device=ctypes.device)\n",
    "    return F.l1_loss(out[correct_type, ctype_id], truth[correct_type])\n",
    "\n",
    "@ct.curry\n",
    "def mae_head_per_ctypes(out, truth, ctypes, ctype_ids):\n",
    "    return torch.cat([mae_head_per_ctype(out, ctypes, truth, ctype_id).unsqueeze(-1) for ctype_id in ctype_ids]).mean()\n",
    "\n",
    "\n",
    "@ct.curry\n",
    "def bidir_combine(out, ctype, truth, src, dst, tid):\n",
    "    fwd = src > dst\n",
    "    bwd = src < dst\n",
    "    outs = (out[fwd] + out[bwd]) / 2.\n",
    "    return outs, ctype[fwd], truth[fwd], src[fwd], dst[fwd], tid[fwd]\n",
    "  \n",
    "  \n",
    "def scalar_metrics(out, ctype, truth, src, dst, tid):\n",
    "    with torch.no_grad():\n",
    "        combined = combine_fn(out, ctype, truth, src, dst, tid)\n",
    "        cout, cctype, ctruth, csrc, cdst, ctid = combined\n",
    "        \n",
    "        metrics = {}\n",
    "        ctype_names = ['1JHC', '1JHN', '2JHC', '2JHH', '2JHN', '3JHC', '3JHH', '3JHN']\n",
    "        maes = []\n",
    "        for ctype_id, label in enumerate(ctype_names):\n",
    "            mae = logMAE_per_ctype_head(cout, cctype, ctruth, ctype_id)\n",
    "            metrics[f'trueLogMAE/{label}'] = mae\n",
    "            maes.append(mae)\n",
    "        metrics['trueLogMAE/mean'] = torch.Tensor(maes).mean()\n",
    "        if not net.training:\n",
    "            print('Epoch', epoch, '-', 'trueLogMAE/mean:', metrics['trueLogMAE/mean'].item())\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9vlH8YD9ENf"
   },
   "source": [
    "# Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 82586,
     "status": "ok",
     "timestamp": 1567683318884,
     "user": {
      "displayName": "Goran Rakocevic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUktqkpJY0qFKtpI6DuRekptNOOnmdXwvcQokR=s64",
      "userId": "16829807710678167120"
     },
     "user_tz": -120
    },
    "id": "npJBrRic4a73",
    "outputId": "5cea59b6-2b7a-4fa3-8a3f-3bbef8dfe93e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter count:  57,833,611\n"
     ]
    }
   ],
   "source": [
    "combine_fn = bidir_combine\n",
    "loss_fn = mae_head_per_ctypes(ctype_ids=TYPES_TO_PROCESS)\n",
    "\n",
    "emb = 48\n",
    "heads = 24\n",
    "bias = False\n",
    "\n",
    "def AttnBlock(in_emb, out_emb):\n",
    "    return nn.Sequential(\n",
    "        EdgeLinear(in_emb, out_emb),\n",
    "        NodeLinear(in_emb, out_emb),\n",
    "        GraphLambda(lambda x: x.view(x.shape[0], heads, -1)),\n",
    "        TripletCat(out='triplet'),\n",
    "        MagicAttn(emb, 3 * emb, heads, attn_key='triplet'),\n",
    "        TripletMultiLinear(emb, emb, emb, heads, bias=bias),\n",
    "        GraphLambda(torch.nn.LayerNorm(heads * emb))\n",
    "    )\n",
    "\n",
    "net = nn.Sequential(\n",
    "    Embed(emb, emb),\n",
    "    AttnBlock(emb, emb * heads), GraphLambda(nn.PReLU()),\n",
    "    GatedResidual(AttnBlock(emb * heads, emb * heads), emb * heads, emb * heads), GraphLambda(nn.PReLU()),\n",
    "    GatedResidual(AttnBlock(emb * heads, emb * heads), emb * heads, emb * heads), GraphLambda(nn.PReLU()),\n",
    "    GatedResidual(AttnBlock(emb * heads, emb * heads), emb * heads, emb * heads), GraphLambda(nn.PReLU()),\n",
    "    GatedResidual(AttnBlock(emb * heads, emb * heads), emb * heads, emb * heads), GraphLambda(nn.PReLU()),\n",
    "    GatedResidual(AttnBlock(emb * heads, emb * heads), emb * heads, emb * heads), GraphLambda(nn.PReLU()),\n",
    "    GatedResidual(AttnBlock(emb * heads, emb * heads), emb * heads, emb * heads), GraphLambda(nn.PReLU()),\n",
    "    GatedResidual(AttnBlock(emb * heads, emb * heads), emb * heads, emb * heads), GraphLambda(nn.PReLU()),\n",
    "    EdgeLinear(emb * heads, 512, bias=True), GraphLambda(nn.PReLU(), node_key=None),\n",
    "    EdgeLinear(512, 8, bias=True)\n",
    ")\n",
    "\n",
    "\n",
    "print('Parameter count: ', hf.format_number(sum([p.numel() for p in net.parameters()])))\n",
    "\n",
    "use_cuda = True\n",
    "if use_cuda:\n",
    "    net = net.cuda()\n",
    "    \n",
    "def noreg(x):\n",
    "    return type(x) == GraphLambda and type(x.fn) == torch.nn.modules.activation.PReLU\n",
    "\n",
    "regs = [p for p in itertools.chain.from_iterable(x.parameters() for x in net if not noreg(x))]\n",
    "noregs = [p for p in itertools.chain.from_iterable(x.parameters() for x in net if noreg(x))]\n",
    "params = [\n",
    "    {'params': regs, 'lr': 0.01, 'weight_decay': 5e-2}, #'betas':[0.99, 0.95]},\n",
    "    {'params': noregs, 'lr': 0.01, 'weight_decay': 0}, # 'betas': [0.99, 0.95]}\n",
    "]\n",
    "\n",
    "optim = LambW(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8CUOEH79PrY"
   },
   "source": [
    "# Setting up the logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 85994,
     "status": "ok",
     "timestamp": 1567683322305,
     "user": {
      "displayName": "Goran Rakocevic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUktqkpJY0qFKtpI6DuRekptNOOnmdXwvcQokR=s64",
      "userId": "16829807710678167120"
     },
     "user_tz": -120
    },
    "id": "z28L10fJ4a8G",
    "outputId": "d772313f-7ba2-4761-83fa-2942bbb0b2ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdrive/My Drive/CHAMPS/runs/scaled-truemlkn-43/train 0\n"
     ]
    }
   ],
   "source": [
    "i = -1\n",
    "start_epoch = 0\n",
    "batch_size = 80\n",
    "\n",
    "if START:\n",
    "  load_checkpoint(START)\n",
    "prefix = settings['TRAIN']['MODEL_NAME']\n",
    "\n",
    "if batch_size is None:\n",
    "    batch_size = 80\n",
    "precision = 'single'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, drop_last=True, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=make_graph_batch(precision=precision))\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    eval_data, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=make_graph_batch(precision=precision))\n",
    "\n",
    "\n",
    "train_log = f'{RUNS}/{prefix}/train'\n",
    "eval_log = f'{RUNS}/{prefix}/eval'\n",
    "swa_log = f'{RUNS}/{prefix}/swa'\n",
    "Path(train_log).mkdir(parents=True, exist_ok=True)\n",
    "Path(eval_log).mkdir(parents=True, exist_ok=True)\n",
    "Path(swa_log).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "checkpoints = Path(CHKPS) / prefix /'train'\n",
    "checkpoints.mkdir(parents=True, exist_ok=True)\n",
    "swa_checkpoints = Path(CHKPS) / prefix /'swa'\n",
    "swa_checkpoints.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(train_log, start_epoch)\n",
    "\n",
    "train_writer = tb.SummaryWriter(log_dir=train_log, filename_suffix='.train')\n",
    "eval_writer = tb.SummaryWriter(log_dir=eval_log, filename_suffix='.eval')\n",
    "swa_writer = tb.SummaryWriter(log_dir=swa_log, filename_suffix='.train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-bM0n7kn9Ydq"
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2929383,
     "status": "error",
     "timestamp": 1567686165706,
     "user": {
      "displayName": "Goran Rakocevic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUktqkpJY0qFKtpI6DuRekptNOOnmdXwvcQokR=s64",
      "userId": "16829807710678167120"
     },
     "user_tz": -120
    },
    "id": "i6bAuJCr4a8T",
    "outputId": "dc184ac8-7912-4596-971f-7d1be096224c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4ea34cfcda40b6a265d9fb2e49011b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24086c2d3b27482b84268e1db84aa23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch: 1', max=954, style=ProgressStyle(description_width='inâ€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-599054a5cbf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_truth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/signal_handling.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mprevious_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 945) is killed by signal: Killed. "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "metric_freq =100\n",
    "start_swa = 60\n",
    "n_swa = 25\n",
    "\n",
    "iper_epoch = len(train_loader)\n",
    "\n",
    "scheduler = CyclicLR(optim, \n",
    "                     base_lr=0.001, max_lr=0.01, cycle_momentum=False,\n",
    "                     step_size_up=15*iper_epoch,\n",
    "                     step_size_down=15*iper_epoch,\n",
    "                     last_epoch=i)\n",
    "\n",
    "for epoch in tqdm(range(start_epoch + 1, start_epoch + epochs + 1)):\n",
    "    train_writer.add_scalar('Optim/Learning rate', optim.param_groups[0]['lr'], i)\n",
    "    train_writer.add_scalar('Optim/Weight Decay', optim.param_groups[0]['weight_decay'], i)\n",
    "    net.train()\n",
    "    for batch in tqdm(train_loader, total=iper_epoch, desc=f'Epoch: {epoch}', leave=False):\n",
    "        i += 1\n",
    "        if use_cuda:\n",
    "            batch.to(next(net.parameters()).device)\n",
    "        optim.zero_grad()\n",
    "        g = net(batch)\n",
    "        out, ctype, truth, src, dst, tid = get_outputs(g, norm_truth=True)\n",
    "        loss = loss_fn(out, truth, ctype)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        if epoch < 31:\n",
    "            scheduler.step()\n",
    "            \n",
    "        if epoch == 35:\n",
    "            optim.param_groups[0]['weight_decay'] = 1e-2\n",
    "            optim.param_groups[0]['lr'] = 1e-3\n",
    "            optim.param_groups[1]['lr'] = 1e-3    \n",
    "    \n",
    "        if (i % metric_freq == 0) or ((i+1) % iper_epoch == 0):            \n",
    "            train_writer.add_scalar('loss', loss.item(), i)\n",
    "            outs = get_outputs(g, denorm_out=True)\n",
    "            outs = [out.detach().cpu() for out in outs]\n",
    "            out, ctype, truth, src, dst, tid = outs\n",
    "            scalars = scalar_metrics(out, ctype, truth, src, dst, tid)\n",
    "            for label, scalar in scalars.items():\n",
    "                train_writer.add_scalar(label, scalar, i)\n",
    "            train_writer.file_writer.flush()\n",
    "    run_eval(net, eval_writer, epoch, i, eval_loader)\n",
    "    torch.save({\n",
    "        'model': net.state_dict(),\n",
    "        'optim': optim.state_dict(),\n",
    "        'prefix': prefix,\n",
    "        'iteration': i,\n",
    "        'epoch': epoch,\n",
    "        'batch_size': batch_size,\n",
    "    }, checkpoints / f'{epoch}.torch')\n",
    "    \n",
    "    if epoch < start_swa:\n",
    "        start_epoch = epoch\n",
    "        continue    \n",
    "    swa_chk = last_n_swa(n_swa, epoch, checkpoints, device='cpu')\n",
    "    assert swa_chk['epoch'] == epoch\n",
    "    net.load_state_dict(swa_chk['model'])\n",
    "    if use_cuda: net.to('cuda')\n",
    "    run_eval(net, swa_writer, epoch, i, eval_loader, label='SWA')\n",
    "    torch.save(swa_chk, swa_checkpoints / f'{epoch}.last_{n_swa}.torch')\n",
    "    net.load_state_dict(torch.load(checkpoints / f'{epoch}.torch')['model'])\n",
    "\n",
    "    to_del = swa_checkpoints / f'{epoch-2}.last_{n_swa}.torch'\n",
    "    if to_del.exists():\n",
    "        to_del.unlink()\n",
    "    \n",
    "    to_del = checkpoints / f'{epoch-n_swa}.torch'\n",
    "    if to_del.exists():\n",
    "        to_del.unlink()\n",
    "    \n",
    "    start_epoch = epoch\n",
    "    \n",
    "train_writer.close()\n",
    "eval_writer.close()\n",
    "swa_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E6bctpu1MoK_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Train.ipynb",
   "provenance": [
    {
     "file_id": "172ljJk_j-dNwnM1k4RoUpR9rXgikHqRj",
     "timestamp": 1563974089105
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
